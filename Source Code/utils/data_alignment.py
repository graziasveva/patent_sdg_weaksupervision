"""
Data Alignment and Splitting Script for SDG-Patent-Paper Pipeline

This script aligns the main dataset (generated by final_dataset_construction.py and to be used in llm_extraction.py)
with the order of the LLM-extracted JSONL files (containing patent_id/paper_id), and splits patent data into training and validation sets.
It then saves the corresponding split embedding arrays for downstream optimization and label generation.

Flow:
1. Load and preprocess the main dataset (CSV), converting SDG and OAID columns to lists.
2. Load and filter LLM extraction JSONL files for patents and papers.
3. Add binary SDG label vectors to the dataset.
4. Perform a stratified multi-label split into train and validation sets.
5. Map the split indices to the full embedding arrays using patent_id (and paper_id if needed).
6. Save the split embedding arrays (function, solution, application, main_text), citation SDG count vectors, and paper SDG label matrix.

Usage:
    python data_alignment.py
    # or import and call main() from another script

This script is a key step to ensure that all downstream ML and optimization steps use properly aligned and reproducible data splits.
"""
import pandas as pd
import numpy as np
import srsly
from ast import literal_eval
from skmultilearn.model_selection import iterative_train_test_split
import os

def load_and_preprocess_dataset(dataset_path):
    """
    Load the main dataset, keep only rows with SDG, and aggregate by patent.
    Returns a DataFrame with columns: patent, oaid (list), SDG (list), SDG_vector (list).
    """
    data = pd.read_csv(dataset_path)
    data_f = data[data.SDG.notnull()][["patent", "oaid", "SDG"]]
    agg_df = data_f.groupby('patent').agg({
        'oaid': lambda x: list(x),
        'SDG': list
    }).reset_index()
    agg_df['SDG_vector'] = agg_df['SDG'].apply(sdg_to_count_vector)
    return agg_df

def load_llm_jsonl(jsonl_path, required_fields=("function", "solution", "application")):
    """
    Load and filter LLM extraction JSONL file to only include entries with all required fields non-empty.
    """
    return [
        doc for doc in srsly.read_jsonl(jsonl_path)
        if all(doc.get(f, "").strip() for f in required_fields)
    ]

def sdg_to_count_vector(sdg_list, num_sdgs=17):
    """
    Convert a list of SDG labels to a binary count vector.
    """
    vector = np.zeros(num_sdgs, dtype=int)
    for sdg in sdg_list:
        sdg_int = int(sdg)
        if 1 <= sdg_int <= num_sdgs:
            vector[sdg_int - 1] += 1
    return vector.tolist()

def add_sdg_vectors(data, num_sdgs=17):
    """
    Add a binary SDG vector column to the DataFrame.
    """
    data['SDG_vector_binary'] = data['SDG'].apply(lambda x: sdg_to_count_vector(x, num_sdgs))
    return data

def stratified_split(data, test_size=0.4, random_seed=42):
    """
    Perform a stratified multi-label split into train and validation sets.
    Returns train and validation DataFrames.
    """
    X_matrix = data.index.values.reshape(-1, 1)
    y_matrix = np.vstack(data["SDG_vector"].apply(np.array))
    X_train, y_train, X_val, y_val = iterative_train_test_split(
        X_matrix, y_matrix, test_size=test_size
    )
    data_train = data[data.index.isin(X_train.reshape(-1,).tolist())].reset_index(drop=True)
    data_val = data[data.index.isin(X_val.reshape(-1,).tolist())].reset_index(drop=True)
    return data_train, data_val

def get_id_to_index_map(jsonl_list, id_field):
    """
    Create a mapping from ID to index for a list of dicts.
    """
    return {doc[id_field]: idx for idx, doc in enumerate(jsonl_list)}

def split_and_save_embeddings(data_split, id_to_index, embedding_paths, output_prefix):
    """
    For a given data split, extract the corresponding indices from the full embedding arrays and save them.
    embedding_paths: dict of {field: path_to_full_embedding}
    output_prefix: prefix for output .npy files (e.g., './vectors/patent_base_train')
    Returns a list of saved file paths.
    """
    indices = [id_to_index[pid] for pid in data_split.patent]
    saved_files = []
    for field, path in embedding_paths.items():
        arr = np.load(path)
        out_path = f"{output_prefix}_{field}.npy"
        np.save(out_path, arr[indices])
        print(f"Saved {out_path} ({arr[indices].shape})")
        saved_files.append(out_path)
    return saved_files

def save_citation_sdg_counts(data_split, patent_id_to_target, output_path):
    """
    Save the citation SDG count vectors for a data split.
    Each row is the SDG_vector for the corresponding patent.
    """
    ids = [literal_eval(patent_id_to_target[patent]) for patent in data_split.patent]
    arr = np.vstack(ids)
    np.save(output_path, arr)
    print(f"Saved {output_path} ({arr.shape})")
    return output_path

def save_paper_sdg_labels(papers, agg_df, output_path):
    """
    Create and save the paper_sdg_labels matrix (num_papers, 17), associating each paper to its SDGs.
    """
    paper_id_list = [int(p["paper_id"]) for p in papers]
    paper_id_to_index = {pid: idx for idx, pid in enumerate(paper_id_list)}
    num_papers = len(paper_id_list)
    num_sdgs = 17
    paper_sdg_labels = np.zeros((num_papers, num_sdgs), dtype=int)
    for _, row in agg_df.iterrows():
        oaid_list = row['oaid']
        sdg_list = row['SDG']
        for paper_id, sdg in zip(oaid_list, sdg_list):
            if paper_id in paper_id_to_index:
                paper_idx = paper_id_to_index[paper_id]
                sdg_idx = int(sdg) - 1
                paper_sdg_labels[paper_idx, sdg_idx] = 1
    np.save(output_path, paper_sdg_labels)
    print(f"Saved {output_path} ({paper_sdg_labels.shape})")
    return output_path

def main():
    """
    Main routine to align, split, and save patent embeddings, citation SDG counts, and paper SDG label matrix.
    Edit the file paths below as needed for your project.
    """
    # --- File paths (edit as needed) ---
    dataset_path = "final_dataset.csv"  # From final_dataset_construction.py
    patent_jsonl_path = "extracted_patents.jsonl"  # From llm_extraction.py
    paper_jsonl_path = "extracted_papers.jsonl"    # From llm_extraction.py
    embedding_paths = {
        "function": "patent_function.npy",
        "solution": "patent_solution.npy",
        "application": "patent_application.npy",
        "main_text": "patent_main_text.npy"
    }
    output_dir = "./vectors"
    os.makedirs(output_dir, exist_ok=True)

    # --- Load and preprocess dataset ---
    print("Loading and preprocessing dataset...")
    agg_df = load_and_preprocess_dataset(dataset_path)
    agg_df.to_csv(os.path.join(output_dir, "dataset.csv"), index=False)

    # --- Load LLM-extracted JSONL files ---
    print("Loading LLM-extracted patent JSONL...")
    patents = load_llm_jsonl(patent_jsonl_path)
    print("Loading LLM-extracted paper JSONL...")
    papers = load_llm_jsonl(paper_jsonl_path)

    # --- Stratified split ---
    print("Performing stratified train/validation split...")
    data_train, data_val = stratified_split(agg_df, test_size=0.4)
    train_csv = os.path.join(output_dir, "data_train.csv")
    val_csv = os.path.join(output_dir, "data_val.csv")
    data_train.to_csv(train_csv, index=False)
    data_val.to_csv(val_csv, index=False)
    print(f"Saved split CSVs to {output_dir}")

    # --- Map patent_id to index in full embeddings ---
    print("Mapping patent IDs to embedding indices...")
    patent_id_to_index = get_id_to_index_map(patents, "patent_id")
    # For citation SDG counts
    patent_id_to_target = {row['patent']: str(row['SDG_vector']) for _, row in agg_df.iterrows()}

    # --- Split and save embeddings for train and val ---
    print("Saving split patent embeddings...")
    saved_train = split_and_save_embeddings(data_train, patent_id_to_index, embedding_paths, os.path.join(output_dir, "patent_base_train"))
    saved_val = split_and_save_embeddings(data_val, patent_id_to_index, embedding_paths, os.path.join(output_dir, "patent_base_val"))

    # --- Save citation SDG count vectors ---
    print("Saving citation SDG count vectors...")
    citation_train = save_citation_sdg_counts(data_train, patent_id_to_target, os.path.join(output_dir, "citation_sdg_counts_train.npy"))
    citation_val = save_citation_sdg_counts(data_val, patent_id_to_target, os.path.join(output_dir, "citation_sdg_counts_val.npy"))

    # --- Save paper SDG label matrix ---
    print("Saving paper SDG label matrix...")
    paper_sdg_labels_path = os.path.join(output_dir, "paper_sdg_labels.npy")
    paper_sdg_labels = save_paper_sdg_labels(papers, agg_df, paper_sdg_labels_path)

    print("\nAlignment and splitting complete.")
    print(f"Train set: {len(data_train)} patents, Validation set: {len(data_val)} patents.")
    print(f"Embeddings, citation SDG counts, and split CSVs saved in {output_dir}")

if __name__ == "__main__":
    main()