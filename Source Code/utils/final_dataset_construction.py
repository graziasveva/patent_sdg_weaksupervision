"""
Script for constructing the final SDG-patent-paper dataset by merging SDG papers, patent-paper citations, and patent metadata.

USAGE & REQUIREMENTS:
---------------------
- Requires CSV files for:
    * SDG publications (e.g., sdg_publication_dataset.csv)
    * Patent-paper citations (reliance.csv, from https://relianceonscience.org/patent-to-paper-citations)
    * OpenAlex DOI mapping (oaid_doi.csv, see below)
- Requires access to MongoDB (for OpenAlex metadata) and PATSTAT (for patent metadata).
- Required Python packages (install with pip): pandas, tqdm, requests, pymongo, concurrent.futures, epo-tipdata (for PatstatClient)

How to get the required CSVs:
- reliance.csv: Download from https://relianceonscience.org/patent-to-paper-citations
- oaid_doi.csv: Can be generated by running the OpenAlex metadata fetch step in this script (requires MongoDB)

Example usage:
    python final_dataset_construction.py \
        --sdg_csv sdg_publication_dataset.csv \
        --reliance_csv reliance.csv \
        --oaid_doi_csv oaid_doi.csv \
        --output_dir ./final_dataset_outputs \
        --mongodb_uri mongodb://localhost:27017/ \
        --mailto your@email.com

This script will:
- Select the top 20,000 cited SDG papers per SDG.
- Merge patent-paper citations with SDG papers using DOI.
- Fetch missing metadata from OpenAlex and MongoDB.
- Merge with patent metadata from PATSTAT.
- Output a final enriched dataset for downstream analysis.
"""


import os
import argparse
import pandas as pd
import requests
import math
import concurrent.futures
import time
from tqdm import tqdm
from pymongo import MongoClient

def load_top_sdg_papers(sdg_csv, top_n=20000):
    """Load SDG publication dataset and keep top N by citation_count for each SDG."""
    sdg = pd.read_csv(sdg_csv)
    top = sdg.sort_values(['SDG', 'citation_count'], ascending=[True, False]) \
             .groupby('SDG') \
             .head(top_n)
    return top

def merge_reliance_with_doi(reliance_csv, oaid_doi_csv):
    """Merge patent-paper citations with OpenAlex DOI mapping."""
    rel = pd.read_csv(reliance_csv)
    doi = pd.read_csv(oaid_doi_csv)
    rel['oaid'] = rel['oaid'].astype(str)
    doi['oaid'] = doi['oaid'].astype(str)
    mx = pd.merge(rel, doi, how='left', on='oaid')
    return mx

def fetch_openalex_metadata(oaid_list, mongodb_uri, mailto, batch_size=50, max_workers=1):
    """Fetch DOI and title for OAIDs from OpenAlex API and store in MongoDB. Generates oaid_doi.csv."""
    client = MongoClient(mongodb_uri)
    db = client['openalex']
    collection = db['works_metadata']
    total_batches = math.ceil(len(oaid_list) / batch_size)
    batches = [oaid_list[i * batch_size: (i + 1) * batch_size] for i in range(total_batches)]
    
    def fetch_batch(batch):
        ids_string = "|".join(batch)
        url = f"https://api.openalex.org/works?filter=openalex_id:{ids_string}&per-page=50&mailto={mailto}"
        try:
            r = requests.get(url)
            r.raise_for_status()
            works = r.json().get("results", [])
            for work in works:
                openalex_id = work.get('id', '').split('/')[-1]
                doi = work.get('doi')
                title = work.get('display_name')
                collection.update_one(
                    {"OpenAlex_ID": openalex_id},
                    {"$set": {"DOI": doi, "title": title}},
                    upsert=True
                )
        except Exception as e:
            print(f"Error fetching batch: {e}")


    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        list(tqdm(executor.map(fetch_batch, batches), total=len(batches)))
    print("All metadata fetched and updated in MongoDB.")

    # Export from MongoDB to CSV
    data = list(collection.find({}, {'_id': 0, 'OpenAlex_ID': 1, 'DOI': 1, 'title': 1}))
    oaid_doi_df = pd.DataFrame(data).rename(columns={'OpenAlex_ID': 'oaid'})
    oaid_doi_df.to_csv("oaid_doi.csv", index=False)
    return oaid_doi_df

def merge_top_sdg_with_reliance(top_sdg, mx):

    """Merge top SDG papers into the reliance dataset using DOI."""

    top_sdg['doi'] = top_sdg['doi'].astype(str)
    mx['doi'] = mx['doi'].astype(str)
    mx2 = pd.merge(mx, top_sdg, how='left', on='doi')
    filt = mx2.groupby('patent').filter(lambda x: x['SDG'].notna().any())
    return filt

def merge_with_full_sdg(filt, sdg_csv):

    """Check for additional SDG citations outside the top N."""

    sdg = pd.read_csv(sdg_csv)
    sdg['doi'] = sdg['doi'].astype(str)
    mx3 = pd.merge(filt, sdg, how='left', on='doi')
    mx3['SDG_x'].fillna(mx3['SDG_y'], inplace=True)
    final_dataset = mx3.rename(columns={'SDG_x': 'SDG'})
    final_dataset0 = final_dataset[['patent', 'oaid', 'SDG', 'Title', 'Abstract']]
    final_dataset0.to_csv('final_dataset0.csv', index=False)
    return final_dataset0

def enrich_with_patstat(final_dataset0_csv, patstat_env='PROD', batch_size=10000):
    
    """Download patent metadata from PATSTAT and merge with the dataset."""

    from epo.tipdata.patstat import PatstatClient

    sdg = pd.read_csv(final_dataset0_csv)
    split_parts = sdg['patent'].str.split("-", expand=True)
    valid_rows = split_parts[2].notna()
    sdg.loc[valid_rows, 'publn_auth'] = split_parts.loc[valid_rows, 0].str.upper()
    sdg.loc[valid_rows, 'publn_nr'] = split_parts.loc[valid_rows, 1]
    sdg.loc[valid_rows, 'publn_kind'] = split_parts.loc[valid_rows, 2].str.upper()

    def build_bigquery_from_df(df):
        values = [(row['publn_auth'], row['publn_nr'], row['publn_kind']) for _, row in df.iterrows()]
        structs = [f"""STRUCT('{a}' AS publn_auth, '{b}' AS publn_nr, '{c}' AS publn_kind)""" for a, b, c in values]
        return ",\n    ".join(structs)

    def run_batched_queries(df, batch_size=batch_size):
        patstat = PatstatClient(env=patstat_env)
        all_results = []
        i = 1
        t = time.time()
        for start in range(0, len(df), batch_size):
            chunk = df.iloc[start:start + batch_size]
            values_block = build_bigquery_from_df(chunk)
            query = f"""
            WITH search_values AS (
              SELECT * FROM UNNEST([
                {values_block}
              ])
            )
            SELECT 
                t.*,
                title.appln_title_lg,
                title.appln_title,
                ab.appln_abstract_lg, 
                ab.appln_abstract,
                main.appln_filing_date,
                main.appln_filing_year
            FROM tls211_pat_publn t
            JOIN search_values sv
                ON (t.publn_auth = sv.publn_auth AND 
                    t.publn_nr = sv.publn_nr AND
                    t.publn_kind = sv.publn_kind)
            LEFT JOIN tls203_appln_abstr ab 
                ON (ab.appln_id = t.appln_id)
            LEFT JOIN tls201_appln main
                ON (main.appln_id = t.appln_id)
            LEFT JOIN tls202_appln_title title
                ON (title.appln_id = t.appln_id);
            """
            results = patstat.sql_query(query, use_legacy_sql=False)
            all_results.extend(results)
            print(f"Batch {i} complete â€“ results: {len(results)}")
            i += 1
        print(f"Total query time: {round((time.time() - t) / 60, 2)} minutes")
        return all_results

    result = run_batched_queries(sdg)
    result_df = pd.DataFrame(result)
    result_df.to_csv("match_elements.csv", index=False)
    return result_df

def merge_patent_metadata(final_dataset0_csv, match_elements_csv, output_csv):
    """Merge patent metadata and textual attributes into the final dataset."""
    f = pd.read_csv(final_dataset0_csv)
    m = pd.read_csv(match_elements_csv)
    m['patent'] = m['publn_auth'] + "-" + m['publn_nr'].astype(str) + m['publn_kind']
    f['patent'] = f['patent'].astype(str)
    fin_dat = pd.merge(f, m, how='left', on='patent')
    fin_dat.to_csv(output_csv, index=False)
    return fin_dat

def main():
    parser = argparse.ArgumentParser(description="Construct the final SDG-patent-paper dataset.")
    parser.add_argument('--sdg_csv', type=str, required=True, help='Path to SDG publication dataset CSV')
    parser.add_argument('--reliance_csv', type=str, required=True, help='Path to patent-paper citation CSV (reliance.csv, from https://relianceonscience.org/patent-to-paper-citations)')
    parser.add_argument('--oaid_doi_csv', type=str, required=True, help='Path to OpenAlex DOI mapping CSV (oaid_doi.csv, can be generated by this script)')
    parser.add_argument('--output_dir', type=str, default='final_dataset_outputs', help='Directory to save outputs')
    parser.add_argument('--mongodb_uri', type=str, default='mongodb://localhost:27017/', help='MongoDB URI for OpenAlex metadata')
    parser.add_argument('--mailto', type=str, required=True, help='Contact email for OpenAlex API')
    parser.add_argument('--patstat_env', type=str, default='PROD', help='PATSTAT environment (default: PROD)')
    parser.add_argument('--top_n', type=int, default=20000, help='Top N cited SDG papers per SDG')
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    os.chdir(args.output_dir)

    print("Step 1: Load top SDG papers...")
    top_sdg = load_top_sdg_papers(args.sdg_csv, top_n=args.top_n)

    print("Step 2: Merge reliance data with DOI mapping...")
    mx = merge_reliance_with_doi(args.reliance_csv, args.oaid_doi_csv)

    print("Step 3: Fetch OpenAlex metadata (if needed)...")
    oaid_list = mx['oaid'].unique().tolist()
    fetch_openalex_metadata(oaid_list, args.mongodb_uri, args.mailto)

    print("Step 4: Merge top SDG with reliance data...")
    filt = merge_top_sdg_with_reliance(top_sdg, mx)

    print("Step 5: Merge with full SDG dataset for additional citations...")
    final_dataset0 = merge_with_full_sdg(filt, args.sdg_csv)

    print("Step 6: Enrich with PATSTAT patent metadata...")
    match_elements_df = enrich_with_patstat('final_dataset0.csv', patstat_env=args.patstat_env)

    print("Step 7: Merge patent metadata and save final dataset...")
    merge_patent_metadata('final_dataset0.csv', 'match_elements.csv', 'final_dataset.csv')
    print(f"Final dataset saved to {os.path.join(args.output_dir, 'final_dataset.csv')}")

if __name__ == "__main__":
    main()
